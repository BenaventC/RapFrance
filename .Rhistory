library(ggrepel)
library(seededlda)
# syntaxis and lexical annotations
theme_set(theme_minimal())
My_Theme = theme(
axis.title.x = element_text(size = 10),
axis.text.x = element_text(size = 7),
axis.text.y = element_text(size = 7),
axis.title.y = element_text(size = 10))
# On lit et fusionne les fichiers
file.names <- dir("./lyrics_2/", pattern = "*.txt", recursive = TRUE, full.names = TRUE)
#lire
text <- lapply(file.names, FUN = read_file)
#concaténer
text <- do.call(rbind, text) %>%
as.data.frame()
#réunir et clarifier les noms de variables : des auteurs et leurs textes
text<-cbind(file.names, text) %>%
mutate(file.names=str_remove(file.names,"./lyrics_2/"),
file.names=str_remove(file.names,"_lyrics.txt")) %>%
rename(Artiste=file.names,
Song=2)
#on splitte les chansons avec une miraculeuse balise
text<-separate_rows(text, Song, sep = "Embed\n")
#on sépare les éléments du champs texte : date, consultations, texte
text1<-text%>%
separate(Song,c("Date", "B"), sep = "\nPageViews")%>%
mutate(Date=str_remove(Date,"\nDate : "),
Date=str_remove(Date,"Date :" ),
Date=str_trim(Date),
Year=str_sub(Date,1,4)) %>%
separate(B,c("Views", "C"), sep = "Lyrics") %>%
separate(Views,c("Views", "Titre"), sep = "\n") %>%
mutate(Views=str_remove(Views, " : "))
#on nettoie les strings
text1$Year<-as.numeric(text1$Year)
text1$Views<-as.numeric(text1$Views)
# surtout les paroles
# mais on ne supprime pas la répétition des refrains
#supprimer les fin de vers, à moins qu'on les replace d'un point ?
#supprimer les mention refrain/ couplet (mais on on pourrait découper)
text1$Paroles<-str_replace_all(text1$C, "\\[.*\\]", "XXXXX")
text1$Paroles<-str_replace_all(text1$Paroles,"\n", ". ")
text1$Paroles<-str_remove_all(text1$Paroles,"XXXXX")
#une balise perdue
text1$Paroles<-str_remove_all(text1$Paroles, "You might also like")
text1$n_words<-str_count(text1$Paroles, "\\S+") #nombre de mots
#detection de langue
library(cld3)
lang<-as.data.frame(detect_language(text1$Paroles))%>%
rename(langue=1)
text1<-cbind(text1, lang)
foo<-text1 %>%
mutate(langue=ifelse(is.na(langue),"Autres", langue)) %>%
mutate(langue=ifelse(langue!="fr" & langue!="en", "Autres",langue))%>%
group_by(langue)%>%
summarise(n=n()) %>%
mutate()
ggplot(foo, aes(x=reorder(langue,n), y=n))+
geom_bar(stat="identity", color="grey")+
coord_flip()+
labs( title = "Distribution des langues détectées (cld3)", x=NULL, y = "Fréquence")
#filtrer le contenu en éliminant l'anglais, pour les dates ça dépend de l'empan. On choisit un pas de 5 ans. On n'éliminera donc pas les plus plus texte.
text1<-text1 %>%
#  filter(Year>1989)%>%
filter(langue!="eng")
ggplot(text1,aes(x=Year))+
geom_bar(, color="grey")+
labs(title="Distribution des textes en fonction du temps", x=NULL, y="Fréquence")
ggsave("RapFArtistesTextY.svg", plot=last_plot(),width = 28, height = 20, units = "cm")
foo <-text1%>%
group_by(Artiste)%>%
summarise(n=n(),
Views=sum(Views,na.rm=NA),
Min =min(Year,na.rm = TRUE),
Max=max(Year,na.rm = TRUE))%>%
filter(Min>0 & Max>0)
ggplot(foo,aes(x=Max, y=Min))+
geom_text_repel(aes(label=Artiste, size=log10(Views+1), color=Min*Max),max.overlaps = 30)+
scale_size(range = c(.5, 3))+
labs(title= "Les artistes du Rap Français : générations et carrières",
subtitle = "La taille est (log)proportionnelle à la popularité (views)",
y="Première chanson (génération)",
x= "Dernière chanson (carrière)")+
theme(legend.position="none")+
xlim(1990, 2025)+
ylim(1985,2025)+
My_Theme # + geom_smooth()
ggsave("RapFArtistes.svg", plot=last_plot(),width = 28, height = 20, units = "cm")
table(text1$Year)
text1$quinc<-NA
text1$quinc[text1$Year<2000] <-"1990"
text1$quinc[text1$Year>1999 & text1$Year<2005] <-"2000"
text1$quinc[text1$Year>2004 & text1$Year<2010] <-"2005"
text1$quinc[text1$Year>2009 & text1$Year<2015] <-"2010"
text1$quinc[text1$Year>2014 & text1$Year<2020] <-"2015"
text1$quinc[text1$Year>2019] <-"2020"
table(text1$quinc)
text1$quinc <- factor(text1$quinc,
levels=c("1990",
"2000",
"2005",
"2010",
"2015",
"2020"))
text1<-text1 %>%filter(!is.na(quinc))
table(text1$quinc)
foo<- text1 %>%
group_by(quinc) %>%
summarise(views_m=mean(Views, na.rm=TRUE),
views_s=sd(Views, na.rm=TRUE))
# la fréquentation des lirycs / old star are gone
ggplot(foo,aes(x=quinc,y=views_m, group=1))+
geom_line()+
ylim(0,100000)
text1<-text1 %>%
select(- 5)
write.csv(text1, "RapLyrics.csv")
ggsave("RapFArtistesPop.svg", plot=last_plot(),width = 28, height = 20, units = "cm")
UD<- readRDS("UD.rds")
library(udpipe)
fr <- udpipe_download_model(language = "french")
udmodel_french <- udpipe_load_model(file = "french-gsd-ud-2.5-191206.udpipe")
UD <- udpipe_annotate(udmodel_french, x=text1$Paroles, trace =200,            parallel.cores = 4)
dim(text1)
UD <- as.data.frame(UD)
saveRDS(UD, "UD.rds")
library(tidyverse)
library(quanteda)
library(quanteda.textmodels)
library(quanteda.textstats)
library(ggwordcloud)
library(quanteda.textplots)
library(tidytext)
library(udpipe)
library(Rtsne)
library(ggrepel)
library(seededlda)
# syntaxis and lexical annotations
My_Theme = theme(
axis.title.x = element_text(size = 10),
axis.text.x = element_text(size = 7),
axis.text.y = element_text(size = 7),
axis.title.y = element_text(size = 10))
theme_set(theme_minimal()+ theme_set(theme_minimal()))
text1<-read.csv("RapLyrics.csv")
ggplot(text1,aes(x=n_words))+
geom_density()+
scale_x_log10()+
labs( title = "Distribution du nombre de mots par texte",
x= "nombre de mots",
y= "densité de probabilité")
#la fonction de calcul de lisibilité
#une fonction impossible
readability<-textstat_readability(text1$Paroles,
measure = c("Flesch",
"meanSentenceLength",
"meanWordSyllables"))
foo<-cbind(text1[,1:10],readability[,2:4])
foo1<-foo %>%
group_by(quinc) %>%
summarise(Flesch=mean(Flesch, na.rm=TRUE),
SentenceLength= mean(meanSentenceLength, na.rm=TRUE),
WordSyllables= mean(meanWordSyllables, na.rm=TRUE)) %>%
pivot_longer(-quinc,names_to="Variable", values_to="Score")
ggplot(foo1,aes(x=quinc, y=Score, group=Variable))+
geom_line(size=1, aes(color=Variable), stat="identity", alpha=0.3)+
geom_smooth(aes(color=Variable))+
facet_wrap(vars(Variable), scale="free", ncol=1)+
labs(title = "Rap Français :  readability", x=NULL, y=NULL)
word<-UD %>%
group_by(token)%>%
summarise(n=n())
View(word)
word<-UD %>%
group_by(lemma)%>%
summarise(n=n())
View(word)
word<-UD %>%
group_by(lemma)%>%
summarise(n=n())
write.csv(word,"word_lemma.csv")
word<-UD %>%
group_by(token)%>%
summarise(n=n())
write.csv(word,"word_token.csv")
word<-UD %>%
group_by(token)%>%
summarise(n=n())%>%mutate(rank=rank(n))
ggplot(word, aes(x=rank, y=n))+geom_point()
word<-UD %>%
group_by(token)%>%
summarise(n=n())%>%mutate(rank=-rank(n))
ggplot(word, aes(x=rank, y=n))+geom_point()
ggplot(word, aes(x=rank, y=n))+geom_point()+scale_y_log10()
ggplot(word, aes(x=rank, y=n))+geom_point()+scale_y_log10()+scale_x_log10()
word<-UD %>%
group_by(token)%>%
summarise(n=n())%>%mutate(rank=rank(-n))
ggplot(word, aes(x=rank, y=n))+geom_point()+scale_y_log10()+scale_x_log10()
ggplot(word, aes(x=rank, y=n))+geom_point()+scale_y_log10()+scale_x_log10()+geom_smooth()
ggplot(word, aes(x=rank, y=n))+geom_point()+scale_y_log10()+scale_x_log10()+geom_smooth(method=lm)
word<-UD %>%
group_by(lemma)%>%
summarise(n=n())%>%mutate(rank=rank(-n))
ggplot(word, aes(x=rank, y=n))+
geom_point()+scale_y_log10()+
scale_x_log10()+
geom_smooth(method=lm)
word<-UD %>%
group_by(lemma)%>%
summarise(n=n())%>%
mutate(rank=rank(-n)
sum=cumsum(n))
word<-UD %>%
group_by(lemma)%>%
summarise(n=n())%>%
mutate(rank=rank(-n),
sum=cumsum(n))
ggplot(word, aes(x=rank, y=sum))+
geom_point()+scale_y_log10()+
scale_x_log10()+
geom_smooth(method=lm)
word<-UD %>%
group_by(lemma)%>%
summarise(n=n())%>% arrange(n)%>%
mutate(rank=rank(-n),
sum=cumsum(n))
View(word)
ggplot(word, aes(x=rank, y=sum))+
geom_point()+scale_y_log10()+
scale_x_log10()+
geom_smooth(method=lm)
ggplot(word, aes(x=n, y=sum))+
geom_point()+scale_y_log10()+
scale_x_log10()+
geom_smooth(method=lm)
ggplot(word, aes(x=n, y=sum))+
geom_point()+scale_y_log10()+
scale_x_log10()+
geom_smooth(method="gam")
corpus<-corpus(text1,id_field = "Titre",text_field = "Paroles")
toks<-tokens(corpus,
remove_punct = TRUE,
remove_symbols=TRUE,
remove_numbers=TRUE)%>%
tokens_remove(stopwords("french"))%>%tokens_tolower()
foo1 <-unlist_tokens(toks)
foo2<-foo1 %>%
group_by(token)%>%
summarise(n=n())%>%
mutate(rank=rank(desc(n)))
ggplot(foo2, aes(x=rank,y=n))+
geom_point(alpha=.2)+geom_smooth(method=lm)+
scale_x_log10()+
scale_y_log10()+
labs(title = "Zipf like")
#with cleaning
dfmat1 <-toks %>% dfm()%>%
dfm_trim(min_termfreq = 1)
textplot_wordcloud(dfmat1, max_words = 200,   max_size = 8)
#keyness
#le cas alkpote
dfmat2 <- dfm(corpus_subset(corpus, Artiste == "Alkpote"),
remove = stopwords("french"), remove_punct = TRUE) %>%
dfm_trim(min_termfreq = 3)
textplot_wordcloud(dfmat2, max_words = 100,max_size = 8)
#le cas Mcsolarr
dfmat2 <- dfm(corpus_subset(corpus, Artiste == "MC Solaar"),
remove = stopwords("french"), remove_punct = TRUE) %>%
dfm_trim(min_termfreq = 3)
textplot_wordcloud(dfmat2, max_words = 200,max_size = 8)
library(tidyverse)
library(quanteda)
library(quanteda.textmodels)
library(quanteda.textstats)
library(ggwordcloud)
library(quanteda.textplots)
library(tidytext)
library(udpipe)
library(Rtsne)
library(ggrepel)
library(seededlda)
# syntaxis and lexical annotations
My_Theme = theme(
axis.title.x = element_text(size = 10),
axis.text.x = element_text(size = 7),
axis.text.y = element_text(size = 7),
axis.title.y = element_text(size = 10))
theme_set(theme_minimal()+ theme_set(theme_minimal()))
text1<-read.csv("RapLyrics.csv") %>%
filter(n_words>100)
library(udpipe)
fr <- udpipe_download_model(language = "french")
udmodel_french <- udpipe_load_model(file = "french-gsd-ud-2.5-191206.udpipe")
UD <- udpipe_annotate(udmodel_french, x=text1$Paroles, trace =200, parallel.cores = 4)
x <- as.data.frame(UD)
library(quanteda)
corp<-text1 %>%select(Artiste, Year, Paroles, quinc, views)
corp<-text1 %>%select(Artiste, Year, Paroles, quinc, Views)
corpus<- corpus(corp$Paroles, docvars =corp)
toks<-tokens(
corpus,
what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_url = TRUE,
remove_separators = TRUE,
split_hyphens = FALSE,
split_tags = FALSE,
include_docvars = TRUE,
padding = FALSE) %>%
tokens_remove(stopwords("french"))
dfm<- dfm(toks)
dfm_tf_idf<-dfm_tfidf(dfm)
df_tf_idf<-as.data.frame(dfm_tf_idf)
dfm<- dfm(toks) %>% dfm_keep( min_nchar = 5)
dfm<- dfm(toks) %>% dfm_trim(min_termfreq = 30)
dfm_tf_idf<-dfm_tfidf(dfm)
df_tf_idf<-as.data.frame(dfm_tf_idf)
foo2<-df_tf_idf%>%
select(-doc_id)%>%
t()
word<-as.data.frame(rownames(foo2))%>%
rename(word=1)
set.seed(42) # Sets seed for reproducibility
tsne_out <- Rtsne(foo2,
initial_dims = 20,
perplexity = 20,
partial_pca=TRUE,
theta=.5,
num_threads=4,
verbose=1,
check_duplicates = FALSE)
tsne_out <- Rtsne(foo2,
initial_dims = 20,
perplexity = 20,
iteration=2000,
partial_pca=TRUE,
theta=.5,
num_threads=4,
verbose=1,
check_duplicates = FALSE)
tsne_out <- Rtsne(foo2,
initial_dims = 20,
perplexity = 20,
iteration=4000,
partial_pca=TRUE,
theta=.5,
num_threads=4,
verbose=1,
check_duplicates = FALSE)
dfm<- dfm(toks) %>% dfm_trim(min_termfreq = 30)
dfm_tf_idf<-dfm_tfidf(dfm)
df_tf_idf<-as.data.frame(dfm_tf_idf)
foo2<-df_tf_idf%>%
select(-doc_id)%>%
t()
word<-as.data.frame(rownames(foo2))%>%
rename(word=1)
set.seed(42) # Sets seed for reproducibility
tsne_out <- Rtsne(foo2,
initial_dims = 20,
perplexity = 20,
#                  iteration=4000, a modifier
partial_pca=TRUE,
theta=.5,
num_threads=4,
verbose=1,
check_duplicates = FALSE)
tsne_out <- Rtsne(foo2,
initial_dims = 20,
perplexity = 20,
max_iter = 2000,
partial_pca=TRUE,
theta=.5,
num_threads=4,
verbose=1,
check_duplicates = FALSE)
tsne_out1<-tsne_out$Y
tsne_out2<-as.data.frame(cbind(word,tsne_out1)) %>%
left_join(words)
tsne_out2<-as.data.frame(cbind(word,tsne_out1)) #%>%  left_join(words)
ggplot(tsne_out2, aes(x=`1`, y=`2` ))+
geom_text(aes(label=word,
size=log(total_word),
alpha=10-log(total_word)))+
theme(legend.position = "none")+
labs(x=NULL, y=NULL)+
scale_size(range = c(.1, 3))
ggplot(tsne_out2, aes(x=`1`, y=`2` ))+
geom_text(aes(label=word,
#                      size=log(total_word),
#                      alpha=10-log(total_word)
))+
theme(legend.position = "none")+
labs(x=NULL, y=NULL)+
scale_size(range = c(.1, 3))
ggsave("tsne.svg",plot=last_plot(), width = 30, height = 20, units = "cm")
library(seededlda)
set.seed(1234)
slda <- textmodel_lda(dfm_tf_idf, 25)
phi<-slda$phi %>%
as.data.frame() %>%
rownames_to_column(var="topic") %>%
pivot_longer(-topic,names_to = "word", values_to = "p")%>%
arrange(topic,p) %>%
group_by(topic)%>%
top_n(40)
ggplot(phi, aes(label=word))+
geom_text_wordcloud(aes(size=p)) +
facet_wrap(vars(topic), ncol=5)+
theme(strip.text.x = element_text(size = 6))+
scale_size(range = c(1, 4))
phi$topic[phi$topic=="topic1"]<-"Traffic"
phi$topic[phi$topic=="topic2"]<-"Love"
phi$topic[phi$topic=="topic3"]<-"Ennui"
phi$topic[phi$topic=="topic4"]<-"Exister socialement"
phi$topic[phi$topic=="topic5"]<-"ouai ouai"
phi$topic[phi$topic=="topic6"]<-"Amour/haine"
phi$topic[phi$topic=="topic7"]<-"Le monde"
phi$topic[phi$topic=="topic8"]<-"Fou/fort"
phi$topic[phi$topic=="topic9"]<-"Mélancolie"
phi$topic[phi$topic=="topic10"]<-"Rapper"
phi$topic[phi$topic=="topic11"]<-"Le Temps qui passe"
phi$topic[phi$topic=="topic12"]<-"délinquance"
phi$topic[phi$topic=="topic13"]<-"Amis/ ennemis"
phi$topic[phi$topic=="topic14"]<-"Onomatopées"
phi$topic[phi$topic=="topic15"]<-"Responsabilité"
phi$topic[phi$topic=="topic16"]<-"respect/gang"
phi$topic[phi$topic=="topic17"]<-"Comprendre"
phi$topic[phi$topic=="topic18"]<-"American"
phi$topic[phi$topic=="topic19"]<-"Dignité"
phi$topic[phi$topic=="topic20"]<-"Violence"
ggplot(phi, aes(label=word))+
geom_text_wordcloud(aes(size=p)) +
facet_wrap(vars(topic), ncol=4)+
theme(strip.text.x = element_text(size =9))+
scale_size(range = c(1, 4))
ggsave("topic_v01.svg",plot=last_plot(), width = 30, height = 20, units = "cm")
theta<-as.data.frame(slda$theta)
topic<-cbind(text1,theta)
foo<-topic %>%
group_by(quinc)%>%
summarise(topic1=mean(topic1, na.rm=TRUE),
topic10=mean(topic10,na.rm=TRUE),
topic16=mean(topic16,na.rm=TRUE),
topic19=mean(topic19,na.rm=TRUE))%>%
pivot_longer(-quinc, names_to = "variable", values_to = "value" )
ggplot(foo, aes(x=quinc,y=value, group=variable ))+
geom_line(stat="identity", aes(color=variable))
ggplot(foo, aes(x=quinc,y=value, group=variable ))+
geom_line(stat="identity", aes(color=variable))+
facet_wrap(vars(variable),ncol=5)
foo<-topic %>%
group_by(quinc)%>%
summarise(topic1=mean(topic1, na.rm=TRUE),
topic2=mean(topic2, na.rm=TRUE),
topic3=mean(topic3, na.rm=TRUE),
topic4=mean(topic4, na.rm=TRUE),
topic5=mean(topic5, na.rm=TRUE),
topic6=mean(topic6, na.rm=TRUE),
topic7=mean(topic7, na.rm=TRUE),
topic8=mean(topic8, na.rm=TRUE),
topic9=mean(topic9, na.rm=TRUE),
topic10=mean(topic10,na.rm=TRUE),
topic11=mean(topic11,na.rm=TRUE),
topic12=mean(topic12,na.rm=TRUE),
topic13=mean(topic13,na.rm=TRUE),
topic14=mean(topic14,na.rm=TRUE),
topic15=mean(topic15,na.rm=TRUE),
topic16=mean(topic16,na.rm=TRUE),
topic17=mean(topic17,na.rm=TRUE),
topic18=mean(topic18,na.rm=TRUE),
topic19=mean(topic19,na.rm=TRUE),
topic20=mean(topic20,na.rm=TRUE),
topic21=mean(topic21,na.rm=TRUE),
topic22=mean(topic22,na.rm=TRUE),
topic23=mean(topic23,na.rm=TRUE),
topic24=mean(topic24,na.rm=TRUE),
topic25=mean(topic25,na.rm=TRUE),
)%>%
pivot_longer(-quinc, names_to = "variable", values_to = "value" )
ggplot(foo, aes(x=quinc,y=value, group=variable ))+
geom_line(stat="identity", aes(color=variable))+
facet_wrap(vars(variable),ncol=5)
ggplot(phi, aes(label=word))+
geom_text_wordcloud(aes(size=p)) +
facet_wrap(vars(topic), ncol=5)+
theme(strip.text.x = element_text(size = 6))+
scale_size(range = c(1, 4))
ggsave("topic_v01.svg",plot=last_plot(), width = 30, height = 20, units = "cm")
ggplot(phi, aes(label=word))+
geom_text_wordcloud(aes(size=p)) +
facet_wrap(vars(topic), ncol=5)+
theme(strip.text.x = element_text(size = 6))+
scale_size(range = c(1, 4))
ggsave("topic_v01.svg",plot=last_plot(), width = 30, height = 20, units = "cm")
phi<-slda$phi %>%
as.data.frame() %>%
rownames_to_column(var="topic") %>%
pivot_longer(-topic,names_to = "word", values_to = "p")%>%
arrange(topic,p) %>%
group_by(topic)%>%
top_n(40)
ggplot(phi, aes(label=word))+
geom_text_wordcloud(aes(size=p)) +
facet_wrap(vars(topic), ncol=5)+
theme(strip.text.x = element_text(size = 6))+
scale_size(range = c(1, 4))
ggsave("topic_v01.svg",plot=last_plot(), width = 30, height = 20, units = "cm")
