library(tidyverse)
library(quanteda)
library(quanteda.textmodels)
library(quanteda.textstats)
library(ggwordcloud)
library(quanteda.textplots)
library(tidytext)
library(udpipe)
library(Rtsne)
library(ggrepel)
library(seededlda)
# syntaxis and lexical annotations
theme_set(theme_minimal())
My_Theme = theme(
axis.title.x = element_text(size = 10),
axis.text.x = element_text(size = 7),
axis.text.y = element_text(size = 7),
axis.title.y = element_text(size = 10))
# On lit et fusionne les fichiers
file.names <- dir("./lyrics_2/", pattern = "*.txt", recursive = TRUE, full.names = TRUE)
#lire
text <- lapply(file.names, FUN = read_file)
#concaténer
text <- do.call(rbind, text) %>%
as.data.frame()
#réunir et clarifier les noms de variables : des auteurs et leurs textes
text<-cbind(file.names, text) %>%
mutate(file.names=str_remove(file.names,"./lyrics_2/"),
file.names=str_remove(file.names,"_lyrics.txt")) %>%
rename(Artiste=file.names,
Song=2)
#on splitte les chansons avec une miraculeuse balise
text<-separate_rows(text, Song, sep = "Embed\n")
#on sépare les éléments du champs texte : date, consultations, texte
text1<-text%>%
separate(Song,c("Date", "B"), sep = "\nPageViews")%>%
mutate(Date=str_remove(Date,"\nDate : "),
Date=str_remove(Date,"Date :" ),
Date=str_trim(Date),
Year=str_sub(Date,1,4)) %>%
separate(B,c("Views", "C"), sep = "Lyrics") %>%
separate(Views,c("Views", "Titre"), sep = "\n") %>%
mutate(Views=str_remove(Views, " : "))
#on nettoie les strings
text1$Year<-as.numeric(text1$Year)
text1$Views<-as.numeric(text1$Views)
# surtout les paroles
# mais on ne supprime pas la répétition des refrains
#supprimer les fin de vers, à moins qu'on les replace d'un point ?
#supprimer les mention refrain/ couplet (mais on on pourrait découper)
text1$Paroles<-str_replace_all(text1$C, "\\[.*\\]", "XXXXX")
text1$Paroles<-str_replace_all(text1$Paroles,"\n", ". ")
text1$Paroles<-str_remove_all(text1$Paroles,"XXXXX")
#une balise perdue
text1$Paroles<-str_remove_all(text1$Paroles, "You might also like")
text1$n_words<-str_count(text1$Paroles, "\\S+") #nombre de mots
#detection de langue
library(cld3)
lang<-as.data.frame(detect_language(text1$Paroles))%>%
rename(langue=1)
text1<-cbind(text1, lang)
foo<-text1 %>%
mutate(langue=ifelse(is.na(langue),"Autres", langue)) %>%
mutate(langue=ifelse(langue!="fr" & langue!="en", "Autres",langue))%>%
group_by(langue)%>%
summarise(n=n()) %>%
mutate()
ggplot(foo, aes(x=reorder(langue,n), y=n))+
geom_bar(stat="identity", color="grey")+
coord_flip()+
labs( title = "Distribution des langues détectées (cld3)", x=NULL, y = "Fréquence")
#filtrer le contenu en éliminant l'anglais, pour les dates ça dépend de l'empan. On choisit un pas de 5 ans. On n'éliminera donc pas les plus plus texte.
text1<-text1 %>%
#  filter(Year>1989)%>%
filter(langue!="eng")
ggplot(text1,aes(x=Year))+
geom_bar(, color="grey")+
labs(title="Distribution des textes en fonction du temps", x=NULL, y="Fréquence")
ggsave("RapFArtistesTextY.svg", plot=last_plot(),width = 28, height = 20, units = "cm")
foo <-text1%>%
group_by(Artiste)%>%
summarise(n=n(),
Views=sum(Views,na.rm=NA),
Min =min(Year,na.rm = TRUE),
Max=max(Year,na.rm = TRUE))%>%
filter(Min>0 & Max>0)
ggplot(foo,aes(x=Max, y=Min))+
geom_text_repel(aes(label=Artiste, size=log10(Views+1), color=Min*Max),max.overlaps = 30)+
scale_size(range = c(.5, 3))+
labs(title= "Les artistes du Rap Français : générations et carrières",
subtitle = "La taille est (log)proportionnelle à la popularité (views)",
y="Première chanson (génération)",
x= "Dernière chanson (carrière)")+
theme(legend.position="none")+
xlim(1990, 2025)+
ylim(1985,2025)+
My_Theme # + geom_smooth()
ggsave("RapFArtistes.svg", plot=last_plot(),width = 28, height = 20, units = "cm")
table(text1$Year)
text1$quinc<-NA
text1$quinc[text1$Year<2000] <-"1990"
text1$quinc[text1$Year>1999 & text1$Year<2005] <-"2000"
text1$quinc[text1$Year>2004 & text1$Year<2010] <-"2005"
text1$quinc[text1$Year>2009 & text1$Year<2015] <-"2010"
text1$quinc[text1$Year>2014 & text1$Year<2020] <-"2015"
text1$quinc[text1$Year>2019] <-"2020"
table(text1$quinc)
text1$quinc <- factor(text1$quinc,
levels=c("1990",
"2000",
"2005",
"2010",
"2015",
"2020"))
text1<-text1 %>%filter(!is.na(quinc))
table(text1$quinc)
foo<- text1 %>%
group_by(quinc) %>%
summarise(views_m=mean(Views, na.rm=TRUE),
views_s=sd(Views, na.rm=TRUE))
# la fréquentation des lirycs / old star are gone
ggplot(foo,aes(x=quinc,y=views_m, group=1))+
geom_line()+
ylim(0,100000)
text1<-text1 %>%
select(- 5)
write.csv(text1, "RapLyrics.csv")
ggsave("RapFArtistesPop.svg", plot=last_plot(),width = 28, height = 20, units = "cm")
UD<- readRDS("UD.rds")
library(udpipe)
fr <- udpipe_download_model(language = "french")
udmodel_french <- udpipe_load_model(file = "french-gsd-ud-2.5-191206.udpipe")
UD <- udpipe_annotate(udmodel_french, x=text1$Paroles, trace =200,            parallel.cores = 4)
dim(text1)
UD <- as.data.frame(UD)
saveRDS(UD, "UD.rds")
library(tidyverse)
library(quanteda)
library(quanteda.textmodels)
library(quanteda.textstats)
library(ggwordcloud)
library(quanteda.textplots)
library(tidytext)
library(udpipe)
library(Rtsne)
library(ggrepel)
library(seededlda)
# syntaxis and lexical annotations
My_Theme = theme(
axis.title.x = element_text(size = 10),
axis.text.x = element_text(size = 7),
axis.text.y = element_text(size = 7),
axis.title.y = element_text(size = 10))
theme_set(theme_minimal()+ theme_set(theme_minimal()))
text1<-read.csv("RapLyrics.csv")
ggplot(text1,aes(x=n_words))+
geom_density()+
scale_x_log10()+
labs( title = "Distribution du nombre de mots par texte",
x= "nombre de mots",
y= "densité de probabilité")
#la fonction de calcul de lisibilité
#une fonction impossible
readability<-textstat_readability(text1$Paroles,
measure = c("Flesch",
"meanSentenceLength",
"meanWordSyllables"))
foo<-cbind(text1[,1:10],readability[,2:4])
foo1<-foo %>%
group_by(quinc) %>%
summarise(Flesch=mean(Flesch, na.rm=TRUE),
SentenceLength= mean(meanSentenceLength, na.rm=TRUE),
WordSyllables= mean(meanWordSyllables, na.rm=TRUE)) %>%
pivot_longer(-quinc,names_to="Variable", values_to="Score")
ggplot(foo1,aes(x=quinc, y=Score, group=Variable))+
geom_line(size=1, aes(color=Variable), stat="identity", alpha=0.3)+
geom_smooth(aes(color=Variable))+
facet_wrap(vars(Variable), scale="free", ncol=1)+
labs(title = "Rap Français :  readability", x=NULL, y=NULL)
word<-UD %>%
group_by(token)%>%
summarise(n=n())
View(word)
word<-UD %>%
group_by(lemma)%>%
summarise(n=n())
View(word)
word<-UD %>%
group_by(lemma)%>%
summarise(n=n())
write.csv(word,"word_lemma.csv")
word<-UD %>%
group_by(token)%>%
summarise(n=n())
write.csv(word,"word_token.csv")
word<-UD %>%
group_by(token)%>%
summarise(n=n())%>%mutate(rank=rank(n))
ggplot(word, aes(x=rank, y=n))+geom_point()
word<-UD %>%
group_by(token)%>%
summarise(n=n())%>%mutate(rank=-rank(n))
ggplot(word, aes(x=rank, y=n))+geom_point()
ggplot(word, aes(x=rank, y=n))+geom_point()+scale_y_log10()
ggplot(word, aes(x=rank, y=n))+geom_point()+scale_y_log10()+scale_x_log10()
word<-UD %>%
group_by(token)%>%
summarise(n=n())%>%mutate(rank=rank(-n))
ggplot(word, aes(x=rank, y=n))+geom_point()+scale_y_log10()+scale_x_log10()
ggplot(word, aes(x=rank, y=n))+geom_point()+scale_y_log10()+scale_x_log10()+geom_smooth()
ggplot(word, aes(x=rank, y=n))+geom_point()+scale_y_log10()+scale_x_log10()+geom_smooth(method=lm)
word<-UD %>%
group_by(lemma)%>%
summarise(n=n())%>%mutate(rank=rank(-n))
ggplot(word, aes(x=rank, y=n))+
geom_point()+scale_y_log10()+
scale_x_log10()+
geom_smooth(method=lm)
word<-UD %>%
group_by(lemma)%>%
summarise(n=n())%>%
mutate(rank=rank(-n)
sum=cumsum(n))
word<-UD %>%
group_by(lemma)%>%
summarise(n=n())%>%
mutate(rank=rank(-n),
sum=cumsum(n))
ggplot(word, aes(x=rank, y=sum))+
geom_point()+scale_y_log10()+
scale_x_log10()+
geom_smooth(method=lm)
word<-UD %>%
group_by(lemma)%>%
summarise(n=n())%>% arrange(n)%>%
mutate(rank=rank(-n),
sum=cumsum(n))
View(word)
ggplot(word, aes(x=rank, y=sum))+
geom_point()+scale_y_log10()+
scale_x_log10()+
geom_smooth(method=lm)
ggplot(word, aes(x=n, y=sum))+
geom_point()+scale_y_log10()+
scale_x_log10()+
geom_smooth(method=lm)
ggplot(word, aes(x=n, y=sum))+
geom_point()+scale_y_log10()+
scale_x_log10()+
geom_smooth(method="gam")
corpus<-corpus(text1,id_field = "Titre",text_field = "Paroles")
toks<-tokens(corpus,
remove_punct = TRUE,
remove_symbols=TRUE,
remove_numbers=TRUE)%>%
tokens_remove(stopwords("french"))%>%tokens_tolower()
foo1 <-unlist_tokens(toks)
foo2<-foo1 %>%
group_by(token)%>%
summarise(n=n())%>%
mutate(rank=rank(desc(n)))
ggplot(foo2, aes(x=rank,y=n))+
geom_point(alpha=.2)+geom_smooth(method=lm)+
scale_x_log10()+
scale_y_log10()+
labs(title = "Zipf like")
#with cleaning
dfmat1 <-toks %>% dfm()%>%
dfm_trim(min_termfreq = 1)
textplot_wordcloud(dfmat1, max_words = 200,   max_size = 8)
#keyness
#le cas alkpote
dfmat2 <- dfm(corpus_subset(corpus, Artiste == "Alkpote"),
remove = stopwords("french"), remove_punct = TRUE) %>%
dfm_trim(min_termfreq = 3)
textplot_wordcloud(dfmat2, max_words = 100,max_size = 8)
#le cas Mcsolarr
dfmat2 <- dfm(corpus_subset(corpus, Artiste == "MC Solaar"),
remove = stopwords("french"), remove_punct = TRUE) %>%
dfm_trim(min_termfreq = 3)
textplot_wordcloud(dfmat2, max_words = 200,max_size = 8)
